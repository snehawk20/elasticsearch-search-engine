{
    "url": "https://en.wikipedia.org/wiki/Collingridge_dilemma",
    "title": "Collingridge dilemma",
    "table_of_contents": [],
    "paragraphs": [
        {
            "title": "",
            "text": "The Collingridge dilemma is a methodological quandary in which efforts to influence or control the further development of technology face a double-bind problem:\n\nThe idea was coined by David Collingridge, The University of Aston, Technology Policy Unit, in his 1980 book The Social Control of Technology.[1] The dilemma is a basic point of reference in technology assessment debates.[2]\n\nIn \"This Explains Everything,\" edited by John Brockman, technology critic Evgeny Morozov explains Collingridge's idea by quoting Collingridge himself: \"When change is easy, the need for it cannot be foreseen; when the need for change is apparent, change has become expensive, difficult, and time-consuming.\"[3]\n\nIn \"The Pacing Problem, the Collingridge Dilemma & Technological Determinism\" by Adam Thierer, a senior research fellow at the Mercatus Center at George Mason University, the Collingridge dilemma is related to the \"pacing problem\" in technology regulation. The \"pacing problem\" refers to the notion that technological innovation is increasingly outpacing the ability of laws and regulations to keep up, first explained in Larry Downes' 2009 book The Laws of Disruption, in which he famously states that \"technology changes exponentially, but social, economic, and legal systems change incrementally\". In Thierer's essay, he tries to correlate these two concepts by saying that \"the 'Collingridge dilemma' is simply a restatement of the pacing problem but with greater stress on the social drivers behind the pacing problem and an implicit solution to 'the problem' in the form of preemptive control of new technologies while they are still young and more manageable.\"[4]\n\nOne solution to Collingridge dilemma is the \"Precautionary Principle.\" Adam Thierer defines it as the belief that new innovations should not be embraced \"until their developers can prove that they will not cause any harm to individuals, groups, specific entities, cultural norms, or various existing laws, norms, or traditions\".[4] If they fail to do so, this innovation should be \"prohibited, curtailed, modified, junked, or ignored\".[5] This definition has been criticized by Kevin Kelly who believe such a principle is ill-defined[4] and is biased against anything new because it drastically elevates the threshold for anything innovative. According to the American philosopher Max More, the Precautionary Principle \"is very good for one thing â€” stopping technological progress...not because it leads in bad directions, but because it leads in no direction at all.\"[5] But the 1992 Rio Declaration on Environment and Development defines the precautionary principle as \"\"Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation.\"[6] So rather than conceived as imposing no change until proof of safety is produced, this definition of the precautionary principle is meant to legitimate protective measures, attempting to avoid the desire of a technology's advocates to delay legislation until irrefutable evidence of harm can be produced.\n\nCollingridge's solution was not exactly the precautionary principle but rather the application of \"Intelligent Trial and Error,\" a process by which decision making power remains decentralized, changes are manageable, technologies and infrastructures are designed to be flexible, and the overall process is oriented towards learning quickly while keeping the potential costs as low as possible. [7] Collingridge advocated ensuring that innovation occurs more incrementally so as to better match the pace of human learning and avoiding technologies whose design was antithetical to an Intelligent Trial and Error process. \n\n"
        }
    ]
}